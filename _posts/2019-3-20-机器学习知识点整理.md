---
layout:     post
title:      机器学习知识点整理
subtitle:   面试
date:       2019-3-20
author:     Colafei
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Git
---


>最后更新于2019年4月3日

## 损失函数

损失函数：表征模型预测值与真实值的不一致程度。记为函数L(Y,f(X))

结构风险函数 = 经验风险项  +  正则项  其中损失函数为经验风险项的重要组成部分

前半部分为经验风险项，后半部分为正则项。

#### 对数损失函数

![](/img/post/20190320/1.png)
P(Y|X)表示样本为Y的概率，数值越大说明预测值与真实值越接近即损失函数应该越小，当P（Y|X）越大的，-logP(Y|X)越小，刚好符合损失函数的定义。



#### 交叉熵损失函数

![](/img/post/20190320/12.png)

``` LR逻辑回归 ```损失函数即为交叉熵损失函数

#### 平方损失函数

![](/img/post/20190320/2.png)

在``` 线性回归 ```中，使用的是平方损失函数，它假设样本和噪声都服从高斯分布（中心极限定理），最后通过极大似然估计（MLE）可以推导出最小二乘式子。

![](/img/post/20190320/3.png)

#### 指数损失函数

![](/img/post/20190320/4.png)
AdaBoost中损失函数为：

![](/img/post/20190320/5.png)
#### Hinge损失函数

用于最大间隔（maximum-margin）分类，其中最有代表性的就是支持向量机SVM。

![](/img/post/20190320/6.png)

其中 y=+1或y=−1，f(x)=wx+b，当为SVM的线性核时。

``` SVM ```中损失函数即为Hinge损失函数：
![](/img/post/20190320/7.png)
![](/img/post/20190320/8.png)

进而变形为：
![](/img/post/20190320/9.png)

#### 0-1损失函数
![](/img/post/20190320/10.png)

#### 绝对值损失函数
![](/img/post/20190320/11.png)

## 逻辑回归LR

逻辑回归假设数据服从``` 伯努利分布 ```,通过``` 极大化似然函数或者最小化交叉熵 ```的方法，运用``` 梯度下降 ```来求解参数，来达到将数据二分类的目的。

**损失函数：**

![](/img/post/20190320/13.png)

``` 逻辑回归的损失函数为什么要使用交叉熵或极大似然函数作为损失函数，而不用MSE呢？ ```
   
&emsp;&emsp;MSE的假设是高斯分布，交叉熵的假设是伯努利分布，而逻辑回归采用的就是伯努利分布；<br />
&emsp;&emsp;交叉熵训练求解参数的速度比较快，发现梯度更新只跟预测值有关，跟sigmoid函数的梯度无关，sigmod函数在它在定义域内的梯度都不大于0.25；而MSE梯度更新跟sigmoid的梯度很相关。

**求解方法：**梯度下降

梯度下降有随机``` 梯度下降 ```，``` 批梯度下降 ```，``` small batch 梯度下降 ```三种方式

&emsp;&emsp;简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。<br />
&emsp;&emsp;随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。<br />
&emsp;&emsp;小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。<br />






	


