---
layout:     post
title:      机器学习知识点整理
subtitle:   面试
date:       2019-3-20
author:     Colafei
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Git
---


>最后更新于2019年4月3日

## 损失函数

损失函数：表征模型预测值与真实值的不一致程度。记为函数L(Y,f(X))

结构风险函数 = 经验风险项  +  正则项  其中损失函数为经验风险项的重要组成部分

前半部分为经验风险项，后半部分为正则项。

#### 对数损失函数

![](/img/post/20190320/1.png)
P(Y|X)表示样本为Y的概率，数值越大说明预测值与真实值越接近即损失函数应该越小，当P（Y|X）越大的，-logP(Y|X)越小，刚好符合损失函数的定义。



#### 交叉熵损失函数

![](/img/post/20190320/12.png)

``` LR逻辑回归 ```损失函数即为交叉熵损失函数

#### 平方损失函数

![](/img/post/20190320/2.png)

在``` 线性回归 ```中，使用的是平方损失函数，它假设样本和噪声都服从高斯分布（中心极限定理），最后通过极大似然估计（MLE）可以推导出最小二乘式子。

![](/img/post/20190320/3.png)

#### 指数损失函数

![](/img/post/20190320/4.png)
AdaBoost中损失函数为：

![](/img/post/20190320/5.png)
#### Hinge损失函数

用于最大间隔（maximum-margin）分类，其中最有代表性的就是支持向量机SVM。

![](/img/post/20190320/6.png)

其中 y=+1或y=−1，f(x)=wx+b，当为SVM的线性核时。

``` SVM ```中损失函数即为Hinge损失函数：
![](/img/post/20190320/7.png)
![](/img/post/20190320/8.png)

进而变形为：
![](/img/post/20190320/9.png)

#### 0-1损失函数
![](/img/post/20190320/10.png)

#### 绝对值损失函数
![](/img/post/20190320/11.png)

## 数据预处理之one hot 独热编码

&emsp;&emsp;One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都有他独立的寄存器位，并且在任意时候只有一位有效。<br />
&emsp;&emsp;One-Hot编码将分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。<br />

例如：<br />
性别特征：[“男”，“女”]<br />
男 => 10 <br />
女 => 01 <br />

祖国特征：[“中国”，“美国”，“法国”]<br />
中国 => 100<br />
美国 => 010<br />
法国 => 001<br />

运动特征：[“足球”，“篮球”，“羽毛球”，“乒乓球”]<br />
足球 => 1000<br />
篮球 => 0100<br />
羽毛球 => 0010<br />
乒乓球 => 0001<br />

所以当一个样本为[“男”，“中国”，“乒乓球”]的时候，完整的特征数字化结果为：<br />

&emsp;&emsp;[1,0,1,0,0,0,0,0,1]<br />

#### 为什么使用one-hot编码处理离散型特征？<br />

&emsp;&emsp;在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，给予的就是欧式空间。<br />
&emsp;&emsp;而我们使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。

## 正则项 范数
引入：<br />
&emsp;&emsp;监督学习的过程可以概括为：最小化误差的同时规则化参数。最小化误差是为了让模型拟合训练数据，规则化参数是为了防止过拟合。参数过多会导致模型复杂度上升，产生过拟合，即训练误差很小，但测试误差很大，
这和监督学习的目标是相违背的。所以需要采取措施，保证模型尽量简单的基础上，最小化训练误差，使模型具有更好的泛化能力（即测试误差也很小）。<br />

**范数规则化有两个作用：**<br />

1）保证模型尽可能的简单，避免过拟合。<br />
2）约束模型特性，加入一些先验知识，例如稀疏、低秩​等。<br />

##### ``` L0范数： ```L0是指向量中非0的元素的个数

![](/img/post/20190320/14.png)

&emsp;&emsp;如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。<br />
&emsp;&emsp;但不幸的是，L0范数的最优化问题是一个NP hard问题，而且理论上有证明，L1范数是L0范数的最优凸近似，因此通常使用L1范数来代替。<br />

##### ``` L1范数： ```向量中各个元素绝对值之和，“稀疏规则算子”（Lasso regularization）

![](/img/post/20190320/15.png)
&emsp;&emsp;L1正则化之所以可以防止过拟合，是因为L1范数就是各个参数的绝对值相加得到的，我们前面讨论了，参数值大小和模型复杂度是成正比的。因此复杂的模型，其L1范数就大，最终导致损失函数就大，说明这个模型就不够好。

##### ``` L2范数： ```向量中所有元素平方和的开方（Ridge Regression，岭回归，“权值衰减”）

![](/img/post/20190320/16.png)
&emsp;&emsp;但与L1范数不一样的是，它不会是每个元素为0，而只是接近于0。越小的参数说明模型越简单，越简单的模型越不容易产生过拟合现象。<br />

``` L1为什么比L2更容易获得稀疏解？ ```<br />
``` 为什么L1稀疏，L2平滑？ ```<br />

从权值更新的角度：<br />

&emsp;&emsp;首先来看看L1和L2的梯度(导数的反方向）：<br />
![](/img/post/20190320/17.png)

我们假定：wi等于不为0的某个正的浮点数，学习速率η 为0.5：<br />

&emsp;&emsp;L1的权值更新公式为wi= wi- η * 1  = wi- 0.5 * 1，也就是说权值每次更新都固定减少一个特定的值(比如0.5)，那么经过若干次迭代之后，权值就有可能减少到0。<br />
&emsp;&emsp;L2的权值更新公式为wi= wi- η * wi= wi- 0.5 * wi，也就是说权值每次都等于上一次的1/2，那么，虽然权值不断变小，但是因为每次都等于上一次的一半，所以很快会收敛到较小的值但不为0。<br />

所以：

&emsp;&emsp;L1能产生等于0的权值，即能够剔除某些特征在模型中的作用（特征选择），即产生稀疏的效果。<br />
&emsp;&emsp;L2可以得迅速得到比较小的权值，但是难以收敛到0，所以产生的不是稀疏而是平滑的效果。<br />

## 逻辑回归LR

逻辑回归假设数据服从``` 伯努利分布 ```,通过``` 极大化似然函数或者最小化交叉熵 ```的方法，运用``` 梯度下降 ```来求解参数，来达到将数据二分类的目的。

**损失函数：**

![](/img/post/20190320/13.png)

``` 逻辑回归的损失函数为什么要使用交叉熵或极大似然函数作为损失函数，而不用MSE呢？ ```
   
&emsp;&emsp;MSE的假设是高斯分布，交叉熵的假设是伯努利分布，而逻辑回归采用的就是伯努利分布；<br />
&emsp;&emsp;交叉熵训练求解参数的速度比较快，发现梯度更新只跟预测值有关，跟sigmoid函数的梯度无关，sigmod函数在它在定义域内的梯度都不大于0.25；而MSE梯度更新跟sigmoid的梯度很相关。

**求解方法：**梯度下降

梯度下降有随机``` 梯度下降 ```，``` 批梯度下降 ```，``` small batch 梯度下降 ```三种方式

&emsp;&emsp;简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。<br />
&emsp;&emsp;随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。<br />
&emsp;&emsp;小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。<br />

**优缺点：** 

优点：<br />
&emsp;&emsp;形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。<br />
&emsp;&emsp;训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。<br />
&emsp;&emsp;资源占用小,尤其是内存。因为只需要存储各个维度的特征值。<br />

缺点：<br />
&emsp;&emsp;准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。<br />
&emsp;&emsp;很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。<br />
&emsp;&emsp;处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。<br />
&emsp;&emsp;逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

#### LR与SVM的区别
&emsp;&emsp;1. 损失函数不同，LR是交叉熵损失，SVM是hinge损失<br />

&emsp;&emsp;2. SVM只考虑部分点，即支持向量，而LR考虑所有的样本<br />

&emsp;&emsp;3. SVM的损失函数自带正则，LR没有，所以SVM是结构风险最小化算法，LR是经验风险最小化。结构风险最小化就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，减小泛化误差。为了达到结构风险最小化的目的，最常用的方法就是添加正则项<br />

&emsp;&emsp;4. LR对异常值敏感，SVM对异常值不敏感<br />

&emsp;&emsp;5. 在训练集较小时，SVM较适用，而LR需要较多的样本<br />

## sql语法

``` join的几种用法总结 ```<br />
&emsp;&emsp;``` left (outer) join ```以left join前面的表为主表，把left join后面的表符合on条件的内容加到主表中，返回记录数跟主表一致，如果没有符合on条件的内容，则将字段显示为NULL<br /> 

&emsp;&emsp;``` right (outer) join ```以right join后面的表为主表，把right join前面的表符合on条件的内容加到主表中，返回记录数和主表一致，如果没有符合on条件的内容，则将字段显示为NULL<br />

&emsp;&emsp;``` inner (outer) join ```就是返回两个表中符合on条件的记录，若不满足条件则不返回<br />

&emsp;&emsp;``` full (outer) join ```以两个表的记录为基准，返回两个表的记录去重之和，关联不上的字段为NULL<br />