---
layout:     post
title:      机器学习知识点整理
subtitle:   面试
date:       2019-3-20
author:     Colafei
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Git
---


>最后更新于2019年4月3日

## 损失函数

损失函数：表征模型预测值与真实值的不一致程度。记为函数L(Y,f(X))

结构风险函数 = 经验风险项  +  正则项  其中损失函数为经验风险项的重要组成部分

前半部分为经验风险项，后半部分为正则项。

#### 对数损失函数

![](/img/post/20190320/1.png)
P(Y|X)表示样本为Y的概率，数值越大说明预测值与真实值越接近即损失函数应该越小，当P（Y|X）越大的，-logP(Y|X)越小，刚好符合损失函数的定义。



#### 交叉熵损失函数

![](/img/post/20190320/12.png)

``` LR逻辑回归 ```损失函数即为交叉熵损失函数

#### 平方损失函数

![](/img/post/20190320/2.png)

在``` 线性回归 ```中，使用的是平方损失函数，它假设样本和噪声都服从高斯分布（中心极限定理），最后通过极大似然估计（MLE）可以推导出最小二乘式子。

![](/img/post/20190320/3.png)

#### 指数损失函数

![](/img/post/20190320/4.png)
AdaBoost中损失函数为：

![](/img/post/20190320/5.png)
#### Hinge损失函数

用于最大间隔（maximum-margin）分类，其中最有代表性的就是支持向量机SVM。

![](/img/post/20190320/6.png)

其中 y=+1或y=−1，f(x)=wx+b，当为SVM的线性核时。

``` SVM ```中损失函数即为Hinge损失函数：
![](/img/post/20190320/7.png)
![](/img/post/20190320/8.png)

进而变形为：
![](/img/post/20190320/9.png)

#### 0-1损失函数
![](/img/post/20190320/10.png)

#### 绝对值损失函数
![](/img/post/20190320/11.png)

## 正则项 范数
引入：<br />
&emsp;&emsp;监督学习的过程可以概括为：最小化误差的同时规则化参数。最小化误差是为了让模型拟合训练数据，规则化参数是为了防止过拟合。参数过多会导致模型复杂度上升，产生过拟合，即训练误差很小，但测试误差很大，
这和监督学习的目标是相违背的。所以需要采取措施，保证模型尽量简单的基础上，最小化训练误差，使模型具有更好的泛化能力（即测试误差也很小）。<br />
**范数规则化有两个作用：**<br />
1）保证模型尽可能的简单，避免过拟合。<br />
2）约束模型特性，加入一些先验知识，例如稀疏、低秩​等。<br />

##### L0范数：L0是指向量中非0的元素的个数

![](/img/post/20190320/14.png)

&emsp;&emsp;如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。<br />
&emsp;&emsp;但不幸的是，L0范数的最优化问题是一个NP hard问题，而且理论上有证明，L1范数是L0范数的最优凸近似，因此通常使用L1范数来代替。<br />

##### L1范数：向量中各个元素绝对值之和，“稀疏规则算子”（Lasso regularization）

![](/img/post/20190320/15.png)
&emsp;&emsp;L1正则化之所以可以防止过拟合，是因为L1范数就是各个参数的绝对值相加得到的，我们前面讨论了，参数值大小和模型复杂度是成正比的。因此复杂的模型，其L1范数就大，最终导致损失函数就大，说明这个模型就不够好。

##### L2范数：向量中所有元素平方和的开方（Ridge Regression，岭回归，“权值衰减”）

![](/img/post/20190320/16.png)
&emsp;&emsp;但与L1范数不一样的是，它不会是每个元素为0，而只是接近于0。越小的参数说明模型越简单，越简单的模型越不容易产生过拟合现象。<br />


## 逻辑回归LR

逻辑回归假设数据服从``` 伯努利分布 ```,通过``` 极大化似然函数或者最小化交叉熵 ```的方法，运用``` 梯度下降 ```来求解参数，来达到将数据二分类的目的。

**损失函数：**

![](/img/post/20190320/13.png)

``` 逻辑回归的损失函数为什么要使用交叉熵或极大似然函数作为损失函数，而不用MSE呢？ ```
   
&emsp;&emsp;MSE的假设是高斯分布，交叉熵的假设是伯努利分布，而逻辑回归采用的就是伯努利分布；<br />
&emsp;&emsp;交叉熵训练求解参数的速度比较快，发现梯度更新只跟预测值有关，跟sigmoid函数的梯度无关，sigmod函数在它在定义域内的梯度都不大于0.25；而MSE梯度更新跟sigmoid的梯度很相关。

**求解方法：**梯度下降

梯度下降有随机``` 梯度下降 ```，``` 批梯度下降 ```，``` small batch 梯度下降 ```三种方式

&emsp;&emsp;简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。<br />
&emsp;&emsp;随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。<br />
&emsp;&emsp;小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。<br />

**优缺点：** 

优点：<br />
&emsp;&emsp;形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。<br />
&emsp;&emsp;训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。<br />
&emsp;&emsp;资源占用小,尤其是内存。因为只需要存储各个维度的特征值。<br />

缺点：<br />
&emsp;&emsp;准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。<br />
&emsp;&emsp;很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。<br />
&emsp;&emsp;处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。<br />
&emsp;&emsp;逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

