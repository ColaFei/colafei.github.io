---
layout:     post
title:      常见模型的总结
subtitle:   面试
date:       2018-10-25
author:     Colafei
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Git
---


>最后更新于10月25日
## 树的bagging与boosting模型

&emsp;&emsp;对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，
h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias)，所以我们会采用深度很深甚至不剪枝的决策树。
&emsp;&emsp;对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何
选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。

## 随机森林

&emsp;&emsp;鉴于决策树容易过拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树:<br />
&emsp;&emsp;我们假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，如果用全样本去训练m棵决策树显然是不可取的，全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的<br />
&emsp;&emsp;产生n个样本的方法采用Bootstrap取样法，这是一种有放回的抽样方法，产生n个样本<br />
&emsp;&emsp;而最终结果采用Bagging的策略来获得，即多数投票机制<br />

### 生成方法
1.从样本集中通过重采样的方式产生n个样本

2.假设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点

3.重复m次，产生m棵决策树

4.多数投票机制来进行预测

### 模型总结

&emsp;&emsp;随机森林是一个比较优秀的模型，它对于多维特征的数据集分类有很高的效率，还可以做特征重要性的选择。<br />
运行效率和准确率较高，实现起来也比较简单。但是在数据噪音比较大的情况下会过拟合，过拟合的缺点对于随机森林来说还是较为致命的。

## GBDT和XGBoost的区别

1. 基分类器的选择：传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

2. 二阶泰勒展开：传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。

3. XGBoost工具支持自定义损失函数，只要函数可一阶和二阶求导。 

4. XGBoost在目标函数里加入了正则项，正则项里包含了树的叶子节点个数，用于控制模型的复杂度，防止过拟合。

5. 列采样：xgboost借鉴了随机森林的做法，支持列采样，可以减少计算。

## GBDT和Adaboost的区别

Adaboost是通过增加那些错分的样本的权重来进行学习；

Gbdt中每一棵树学的是之前所有树结论和的残差，也就是梯度。

## 逻辑回归LR 
#### 模型的使用场景

用于分类场景， 尤其是因变量是二分类， 比如垃圾邮件判断（是/否垃圾邮件），是否患某种疾病（是/否）, 广告是否点击等场景。

#### 模型的优缺点

``` 缺点：```

需要大量样本，因为最大似然估计在低样本量的情况下不如最小二乘法有效；

对变量的相关性比较敏感，需要对自变量进行相关性分析，提出线性相关的变量；

为了防止过拟合和欠拟合，模型构建的变量必须是显著的。

``` 优点：```

模型比较简单，好理解，实现大规模线性分类时比较方便


## GBDT 
#### 原理

gbdt并基于gradient boosting学习到一组决策树，即CART。

#### 模型的使用场景

GBDT用来做回归预测，调整后也可以用于分类（设定阈值，大于阈值为正例，反之为负例）

## XGBoost 

#### 原理

一个基于预排序的boosting决策树模型，在对数据进行训练之前，预先对数据进行了预排序，叶子生长方式是level-wise。

#### 模型的使用场景

可以用来做回归，也可以做分类；xgboost的基学习器也可以是线性的，可以做线性回归，线性分类，多分类等。

#### 防止过拟合

借鉴了随机森林的思想，可以使用列采样来防止过拟合。


## LightGBM(lgb)
#### 原理

#### 模型的使用场景

``` 优点：```<br />
1.直接支持类别特征(Categorical Feature)，不需要进行0/1展开。<br />
&emsp;&emsp;相对0/1展开的解决方案，速度快非常多，且精度一致。大多数机器学习工具都无法直接支持类别特征作为输入，一般需要转换成多维0/1特征，带来计算和内存上的额外消耗。LightGBM增加了针对于类别特征的决策规则，
这在决策树上也很好实现。主要的思想是，在对类别特征计算分割增益的时候，不是按照数值特征那样由一个阈值进行切分，而是直接把其中一个类别当成一类，其他的类别当成另一类。这实际上与0/1展开的效果是一样的。<br />
2.LightGBM 选择了基于 histogram 的决策树算法。<br />
&emsp;&emsp;相比于另一个主流的算法 pre-sorted（xgboost），histogram 在内存消耗和计算代价上都有不少优势。<br />
3.在 histogram 算法之上， LightGBM 进行进一步的优化。首先它抛弃了大多数 GBDT 工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。<br />
&emsp;&emsp; level-wise 过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，不容易过拟合。但实际上level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。
因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大(一般也是数据量最大)的一个叶子，然后分裂，如此循环。因此同 level-wise 相比，在分裂次数相同的情况下，
leaf-wise 可以降低更多的误差，得到更好的精度。leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。
	


