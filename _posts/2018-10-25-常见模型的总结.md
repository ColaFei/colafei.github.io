---
layout:     post
title:      常见模型的总结
subtitle:   面试
date:       2018-10-25
author:     Colafei
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Git
---


>最后更新于10月25日

## GBDT和XGBoost的区别

1. 基分类器的选择：传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

2. 二阶泰勒展开：传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。

3. XGBoost工具支持自定义损失函数，只要函数可一阶和二阶求导。 

4. XGBoost在目标函数里加入了正则项，正则项里包含了树的叶子节点个数，用于控制模型的复杂度，防止过拟合。

5. 列采样：xgboost借鉴了随机森林的做法，支持列采样，可以减少计算。

## GBDT和Adaboost的区别

Adaboost是通过增加那些错分的样本的权重来进行学习；

Gbdt中每一棵树学的是之前所有树结论和的残差，也就是梯度。

## 逻辑回归LR 
#### 模型的使用场景

用于分类场景， 尤其是因变量是二分类， 比如垃圾邮件判断（是/否垃圾邮件），是否患某种疾病（是/否）, 广告是否点击等场景。

#### 模型的优缺点

``` 缺点：```

需要大量样本，因为最大似然估计在低样本量的情况下不如最小二乘法有效；

对变量的相关性比较敏感，需要对自变量进行相关性分析，提出线性相关的变量；

为了防止过拟合和欠拟合，模型构建的变量必须是显著的。

``` 优点：```

模型比较简单，好理解，实现大规模线性分类时比较方便


## GBDT 
#### 原理

gbdt并基于gradient boosting学习到一组决策树，即CART。

#### 模型的使用场景

GBDT用来做回归预测，调整后也可以用于分类（设定阈值，大于阈值为正例，反之为负例）

## XGBoost 

#### 原理

一个基于预排序的boosting决策树模型，在对数据进行训练之前，预先对数据进行了预排序，叶子生长方式是level-wise，。

#### 模型的使用场景

可以用来做回归，也可以做分类；xgboost的基学习器也可以是线性的，可以做线性回归，线性分类，多分类等。

#### 防止过拟合

借鉴了随机森林的思想，可以使用列采样来防止过拟合。


## LightGBM(lgb)
#### 原理

#### 模型的使用场景






	


