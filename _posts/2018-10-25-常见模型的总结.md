---
layout:     post
title:      常见模型的总结
subtitle:   面试
date:       2018-10-25
author:     Colafei
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Git
---


>最后更新于10月25日

## 逻辑回归LR 

#### 回归与分类的根本区别：在于输出空间是否为一个度量空间

&emsp;&emsp;对于``` 回归 ```问题：其输出空间B是一个度量空间。即回归问题输出空间定义了一个度量d = ρ（Ytrue,Ypred）去衡量预测值和真实值的误差大小
例如：预测一瓶可乐的价格（真实价格为5元）为6元时，误差为1，预测为7元时，误差为2.这两个预测结果是不同的，是有度量定义来衡量这种差异的（于是有了均方误差这类误差函数）

&emsp;&emsp;对于``` 分类 ```问题：其输出空间B不是一个度量空间。在分类问题中，只有分类“正确”，“错
误”之分，至于错误时将class 5 分到class 6 还是 class 7并没有区别，都是在error counter上+1

#### 模型的使用场景

用于分类场景， 尤其是因变量是二分类， 比如垃圾邮件判断（是/否垃圾邮件），是否患某种疾病（是/否）, 广告是否点击等场景。

#### 特征处理

一般要对特征进行``` 归一化 ```处理，可以提高模型的收敛速度和精度

#### 模型的优缺点

``` 缺点：```

&emsp;&emsp;1.需要大量样本，因为最大似然估计在低样本量的情况下不如最小二乘法有效；<br />
&emsp;&emsp;2.对变量的相关性比较敏感，需要对自变量进行相关性分析，提出线性相关的变量；<br />
&emsp;&emsp;3.为了防止过拟合和欠拟合，模型构建的变量必须是显著的。<br />

``` 优点：```

&emsp;&emsp;模型比较简单，好理解，实现大规模线性分类时比较方便

## 如何处理不均衡样本

&emsp;&emsp;1.负采样<br />
&emsp;&emsp;2.正采样<br />
&emsp;&emsp;3.代价敏感学习：分类器和决策规则必须根据目标进行设置。按错误的代价给予不同的权重（在目标函数上体现，自定义目标函数）<br />

## 过拟合与欠拟合

过拟合的原因：<br />
&emsp;&emsp;1.训练数据量太少<br />
&emsp;&emsp;2.训练模型过度导致模型非常复杂<br />
&emsp;&emsp;3.数据有噪声<br />

解决方法：<br />
&emsp;&emsp;1.为模型添加正则项，控制模型复杂程度<br />
&emsp;&emsp;2.增加训练数据集<br />
&emsp;&emsp;3.集成学习，融合多个模型<br />
&emsp;&emsp;4.Dropout：用于神经网络训练中，让一些隐层节点以一定的概率停止工作，即使节点值为0，这样可以减少隐层节点的相互作用，使模型泛化性更强<br />

## 树的bagging与boosting模型

&emsp;&emsp;对于``` Bagging ```算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，
h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias)，所以我们会采用深度很深甚至不剪枝的决策树。<br />
&emsp;&emsp;对于``` Boosting ```来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias），所以对于每个基分类器来说，问题就在于如何
选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。

## 随机森林

&emsp;&emsp;鉴于决策树容易过拟合的缺点，随机森林采用``` 多个决策树的投票机制 ```来改善决策树:<br />
&emsp;&emsp;我们假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，如果用全样本去训练m棵决策树显然是不可取的，全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的<br />
&emsp;&emsp;产生n个样本的方法采用Bootstrap取样法，这是一种有放回的抽样方法，产生n个样本<br />
&emsp;&emsp;而最终结果采用Bagging的策略来获得，即多数投票机制<br />

&emsp;&emsp;一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting

#### 生成方法
1.从样本集中通过重采样的方式产生n个样本<br />
2.假设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点<br />
3.重复m次，产生m棵决策树<br />
4.多数投票机制来进行预测<br />

#### 模型总结

&emsp;&emsp;随机森林是一个比较优秀的模型，它对于多维特征的数据集分类有很高的效率，还可以做特征重要性的选择。<br />
&emsp;&emsp;运行效率和准确率较高，实现起来也比较简单。但是在数据噪音比较大的情况下会过拟合，过拟合的缺点对于随机森林来说还是较为致命的。

## 决策树的boosting集成

> 让错分的样本权重越来越大，使它们被后边的树更加重视

## GBDT
> 每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量

> ``` 特征重要度 ```：通过计算在每棵树中按这个特征分裂之后损失的减少值，最后取平均值

&emsp;&emsp;GBDT中的树都是回归树，用来做回归预测，也可以用于分类<br />
&emsp;&emsp;Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。
所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少。<br />
&emsp;&emsp;Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，
它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight，但和Gradient并没有关系。

**1、 在回归与多分类任务中的主要区别**

在于``` 损失函数不同 ```造成的``` 算法初始化不同 ```以及``` 叶节点取值和表示含义的不同 ```。

损失函数：

&emsp;&emsp;回归任务：常用均方损失函数、绝对值损失函数和Huber损失函数(前面两者的折中)<br />
&emsp;&emsp;多分类任务：采用多类逻辑损失函数(二分类是对数损失函数)<br />

预测值：

&emsp;&emsp;对于回归, 定义好损失函数之后, GBDT算法的预测值表示与真实值的偏差<br />
&emsp;&emsp;多分类中的预测值则表示预测值为真实值的概率<br />

ps: 对于多分类, GBDT采用一对多策略, 训练过程中的主要区别是多分类中多了一层内部for循环(k个类别都各自拟合完一棵树之后再拟合下一棵树, 一共拟合M轮, 最终有M·K颗树, 最后使用softmax来计算最终的类别概率。

**2、 GBDT防止过拟合的几种方法**

* 控制tree的棵树，即迭代次数M。<br />
* 控制学习率。根据经验，已经发现使用较小的学习率（例如）会使模型的泛化能力显着提高。<br />
* 随机采样迭代。类bagging方法，经验来说随机采样率f在0.5<=f<=0.8比较合适，即可以帮助避免过拟合又可以提高训练速度。<br />
* 控制叶子节点中的最少样本个数。<br />
* 惩罚树的复杂性（复杂性定义为叶子数占树所有节点的比例），用一个后验剪枝算法来对loss和树的复杂度进行联合优化，该方法为去掉那些降低loss幅度小于指定阈值的分支。<br />
* 加入正则：特征的正向正则或者负向正则。在分裂的时候，除了满足最小平方差之外，要保证左子树的lable平均值小于右子树的lable平均值，反之为负向正则。<br />

## XGBoost
> 能自动利用cpu的多线程，而且适当改进了gradient boosting，加了剪枝，控制了模型的复杂程度

> ``` 特征重要度 ```：通过该特征每棵树中分裂次数的和去计算的，比如这个特征在第一棵树分裂1次，第二棵树2次……，那么这个特征的得分就是(1+2+…)。

传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。（xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。）

它的并行是在特征粒度上的：我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，
后面的迭代中重复地使用这个结构，大大减小计算量。 这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

可并行的近似直方图算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。 当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提
出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

## GBDT和XGBoost的区别

1. 基分类器的选择：传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

2. 二阶泰勒展开：传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。

3. XGBoost工具支持自定义损失函数，只要函数可一阶和二阶求导。 

4. XGBoost在目标函数里加入了正则项，正则项里包含了树的叶子节点个数，用于控制模型的复杂度，防止过拟合。

5. 列采样：xgboost借鉴了随机森林的做法，支持列采样，可以减少计算。

## GBDT和Adaboost的区别

Adaboost是通过增加那些错分的样本的权重来进行学习；

Gbdt中每一棵树学的是之前所有树结论和的残差，也就是梯度。




## Lightgbm
#### LightGBM与XGBoost的区别

关于数据分割点：

``` XGBoost ```使用的是pre-sorted算法（对所有特征都按照特征的数值进行预排序，在遍历分割点的时候用O(data)的代价找到一个特征上的最好分割点），能够更精确的找到数据分隔点

``` LightGBM ```使用的是histogram算法，占用的内存更低，数据分隔的复杂度更低

关于决策树的生长策略：

``` XGBoost ```采用的是level-wise生长策略，能够同时分裂同一层的叶子，从而进行多线程优化，不容易过拟合；但不加区分的对待同一层的叶子，带来了很多没必要的开销（因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂）

``` LightGBM ```采用leaf-wise生长策略，每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环；但会生长出比较深的决策树，产生过拟合（因此LightGBM 在leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合）。

另一个比较巧妙的优化是 histogram 做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。

关于特征数据：

``` XGBoost ```本身无法处理分类变量，而是像随机森林一样，只接受数值数据，因此在将分类数据传入 XGBoost 之前，必须通过各种编码方式：例如标记编码、均值编码或独热编码对数据进行处理

``` LightGBM ```可以处理类别变量

#### 模型的使用场景

``` 优点：```<br />
1.直接支持类别特征(Categorical Feature)，不需要进行0/1展开。<br />
&emsp;&emsp;相对0/1展开的解决方案，速度快非常多，且精度一致。大多数机器学习工具都无法直接支持类别特征作为输入，一般需要转换成多维0/1特征，带来计算和内存上的额外消耗。LightGBM增加了针对于类别特征的决策规则，
这在决策树上也很好实现。主要的思想是，在对类别特征计算分割增益的时候，不是按照数值特征那样由一个阈值进行切分，而是直接把其中一个类别当成一类，其他的类别当成另一类。这实际上与0/1展开的效果是一样的。<br />
2.LightGBM 选择了基于 histogram 的决策树算法。<br />
&emsp;&emsp;相比于另一个主流的算法 pre-sorted（xgboost），histogram 在内存消耗和计算代价上都有不少优势。<br />
3.在 histogram 算法之上， LightGBM 进行进一步的优化。首先它抛弃了大多数 GBDT 工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。<br />
&emsp;&emsp; level-wise 过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，不容易过拟合。但实际上level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。
因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大(一般也是数据量最大)的一个叶子，然后分裂，如此循环。因此同 level-wise 相比，在分裂次数相同的情况下，
leaf-wise 可以降低更多的误差，得到更好的精度。leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。
	


